model:
    embed:
        model_name: BAAI/bge-m3
    rerank:
        model_name: BAAI/bge-reranker-v2-m3
        sim_top_k: 2
    llm:
        api: http://localhost:8000/generate
        max_new_tokens: 8192
        temperature: 0
        dir: /workspace/backup/local_model/gemma-2-9b-it/

logging:
    level: 20
    format: "{level} | {time:YYYY-MM-DD at HH:mm:ss} | {name}: [{correlation_id}] - {message}"
    file: "logs/seoul-ai-manual_gemma-2-9b-it.log"

qdrant:
    host: 114.110.128.123
    port: 6333

redis:
    host: localhost
    port: 6379
    db: 0
    password: null

engine:
    collection: "seoul"
    template:
        context: |
            아래에 문맥 정보가 있습니다.
            ----------------------------
            {context_str}
            ----------------------------
            주어진 문맥 정보를 활용하여 질문에 답변하세요.
            질문: {query_str}
        no_context: |
            질문에 답변하세요.
            질문: {query_str}
        condense: |
            아래에 주어진 대화가 있습니다:
            ---------------------------
            user: {user_query}
            assistant: {assistant_response}
            user: {current_query}
            ---------------------------
            위의 대화를 기반으로 현재 사용자의 새 질문을 condensed question으로 생성하세요.
            condensed_question:
    sim_top_k: 10
    sim_cutoff: -1